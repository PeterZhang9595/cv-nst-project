# vgg参数
0.weight torch.Size([64, 3, 3, 3])
0.bias torch.Size([64])
2.weight torch.Size([64, 64, 3, 3])
2.bias torch.Size([64])
5.weight torch.Size([128, 64, 3, 3])
5.bias torch.Size([128])
7.weight torch.Size([128, 128, 3, 3])
7.bias torch.Size([128])
10.weight torch.Size([256, 128, 3, 3])
10.bias torch.Size([256])
12.weight torch.Size([256, 256, 3, 3])
12.bias torch.Size([256])
14.weight torch.Size([256, 256, 3, 3])
14.bias torch.Size([256])
16.weight torch.Size([256, 256, 3, 3])
16.bias torch.Size([256])
19.weight torch.Size([512, 256, 3, 3])
19.bias torch.Size([512])
21.weight torch.Size([512, 512, 3, 3])
21.bias torch.Size([512])
23.weight torch.Size([512, 512, 3, 3])
23.bias torch.Size([512])
25.weight torch.Size([512, 512, 3, 3])
25.bias torch.Size([512])
28.weight torch.Size([512, 512, 3, 3])
28.bias torch.Size([512])
30.weight torch.Size([512, 512, 3, 3])
30.bias torch.Size([512])
32.weight torch.Size([512, 512, 3, 3])
32.bias torch.Size([512])
34.weight torch.Size([512, 512, 3, 3])
34.bias torch.Size([512])

# vgg.children()
## layer1
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
## layer2
Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
## layer3
Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
## layer4
Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
## layer5
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
ReLU(inplace=True)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

# debug小发现
normalization层的使用位置。